{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json \n",
    "\n",
    "#plot confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import stratified k fold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, fbeta_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import re\n",
    "\n",
    "\n",
    "import statistics\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot roc curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "#plot roc curve\n",
    "def plot_roc_curve(y_test, y_pred,label):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic: '+label)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix function\n",
    "def plot_confusion_matrix(y_test, y_pred, label):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure()\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\")\n",
    "    plt.title('Confusion Matrix: '+label)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for metrics\n",
    "def metrics(X,y,y_test, y_pred, modell = None, crossval = True, parameters = None):\n",
    "\n",
    "\n",
    "    print(\"----- Metric for 80/20 Split -----\")\n",
    "    print(\"Accuracy score:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Recall score:\", recall_score(y_test, y_pred))\n",
    "    print(\"Precision score:\", precision_score(y_test, y_pred))\n",
    "    print(\"fbeta score:\", fbeta_score(y_test, y_pred, beta=2))\n",
    "    print(\"----- Metric for Cross Validation -----\")\n",
    "\n",
    "    if crossval == True and smote == False:\n",
    "        modell.fit(X,y)\n",
    "        \n",
    "        print(\"Crossval score:\", statistics.mean(cross_val_score(modell ,X, y, scoring=f2,cv = cv )))\n",
    "        # print all the scores\n",
    "        print(\"Crossval score:\", cross_val_score(modell ,X, y, scoring=f2,cv = cv ))\n",
    "    \n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append metrics to dictionary\n",
    "def metrics_dict_non_opt(y_test, y_pred, label, modell = None):\n",
    "    Non_opt_results[\"Algo_name\"].append(label)\n",
    "    Non_opt_results[\"Recall\"].append(recall_score(y_test, y_pred))\n",
    "    Non_opt_results[\"Precision\"].append(precision_score(y_test, y_pred))\n",
    "    Non_opt_results[\"Accuracy\"].append(accuracy_score(y_test, y_pred))\n",
    "    Non_opt_results[\"F2\"].append(fbeta_score(y_test, y_pred,beta=2))\n",
    "    Non_opt_results[\"F2-CV\"].append(statistics.mean(cross_val_score(modell ,X, y, scoring=f2,cv = cv )))\n",
    "\n",
    "\n",
    "def metrics_dict_opt(y_test, y_pred, label, modell = None):\n",
    "    Opt_results[\"Algo_name\"].append(label)\n",
    "    Opt_results[\"Recall\"].append(recall_score(y_test, y_pred))\n",
    "    Opt_results[\"Precision\"].append(precision_score(y_test, y_pred))\n",
    "    Opt_results[\"Accuracy\"].append(accuracy_score(y_test, y_pred))\n",
    "    Opt_results[\"F2\"].append(fbeta_score(y_test, y_pred,beta=2))\n",
    "    Opt_results[\"F2-CV\"].append(statistics.mean(cross_val_score(modell ,X, y, scoring=f2,cv = cv )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for crossvalidation and grid search measurement\n",
    "def f2_measure(y_true, y_pred):\n",
    "\treturn fbeta_score(y_true, y_pred, beta=2)\n",
    "f2 = make_scorer(f2_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load german_data_cleaned.csv\n",
    "german_data = pd.read_csv('german_data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Feauture und Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split features and target\n",
    "X = german_data.drop(['Target'], axis=1)\n",
    "y = german_data['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Kreuzvalidierung\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary with algo names and score metrics for later analysis\n",
    "\n",
    "Non_opt_results = {\"Algo_name\": [], \"Recall\": [],\"Precision\": [],\"Accuracy\": [], \"F2\": [], \"F2-CV\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test set 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling Non-Optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use logistic regression to predict target variable\n",
    "# create logistic regression model with standard parameters\n",
    "\n",
    "def logistic_regression(X_train, X_test, y_train, y_test, hyperopt = False, parameters=dict()):\n",
    "    logreg = LogisticRegression(random_state=42, **parameters)\n",
    "    \n",
    "   \n",
    "    if hyperopt == False:\n",
    "        # fit the model\n",
    "        logreg.fit(X_train, y_train)\n",
    "\n",
    "        # predict the target variable\n",
    "        y_pred = logreg.predict(X_test)\n",
    "        \n",
    "        # print accuracy score and recall score\n",
    "        metrics(X,y,y_test, y_pred, crossval=True, modell=logreg)\n",
    "\n",
    "        print(X_train.shape ,y_train.shape)\n",
    "\n",
    "        # add algo name and recall score to dictionary\n",
    "        metrics_dict_non_opt(y_test, y_pred, \"Logistic Regression\", logreg)\n",
    "        plot_roc_curve(y_test, y_pred,\"Logistic Regression\")\n",
    "        plot_confusion_matrix(y_test,y_pred,\"Logistic Regression\")\n",
    "    else:\n",
    "\n",
    "        return logreg\n",
    "\n",
    "    return logreg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, X_test, y_train, y_test, hyperopt = False, parameters=dict()):\n",
    "    rf = RandomForestClassifier(random_state=42,**parameters)\n",
    "\n",
    "    if hyperopt == False:\n",
    "        rf.fit(X_train, y_train)\n",
    "        # predict the target variable\n",
    "        y_pred = rf.predict(X_test)\n",
    "        # print accuracy score and recall score\n",
    "        metrics(X,y,y_test, y_pred, crossval=True, modell=rf)\n",
    "\n",
    "\n",
    "        metrics_dict_non_opt(y_test, y_pred, \"Random forest\", rf)\n",
    "\n",
    "\n",
    "        # show feature importance for random forest\n",
    "\n",
    "        importances = rf.feature_importances_\n",
    "        # plot importances\n",
    "        plt.figure(figsize=(10,10))\n",
    "        # order importances\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        sns.barplot(x=importances[indices], y=X_train.columns, orient='h')\n",
    "        plt.xlabel('Feature Importance')\n",
    "        plt.ylabel('Features')\n",
    "        plt.title('Feature Importance')\n",
    "        plt.show()\n",
    "\n",
    "        plot_roc_curve(y_test, y_pred,\"Random Forest\")\n",
    "        plot_confusion_matrix(y_test,y_pred,\"Random Forest\")\n",
    "    else:\n",
    "        return rf\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuronal Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalerweise würde man die Daten für das Training mit einem Neuronalen Netz mithilfe eines scaler nomalisieren bzw. skalieren.\n",
    "Dies wurde auch mithilfe des minmax scalers und dem standardscaler versucht. Allerdings hatten beide einen negativen Einfluss auf die Ergebnisse des Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use neuro network to predict target variable\n",
    "# als baseline werden die Standardparameter verwendet\n",
    "# normalisieren\n",
    "# fit the model\n",
    "\n",
    "# import standard scaler from sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def neuronal_network(X_train, X_test, y_train, y_test, hyperopt = False, parameters=dict()):\n",
    "    \n",
    "    if hyperopt == False:\n",
    "        nn = MLPClassifier(hidden_layer_sizes=(100), random_state=42, **parameters)\n",
    "        nn.fit(X_train, y_train)\n",
    "        # predict the target variable\n",
    "        y_pred = nn.predict(X_test)\n",
    "        # print accuracy score and recall score\n",
    "        metrics(X,y,y_test, y_pred, crossval=True, modell=nn)\n",
    "\n",
    "\n",
    "        metrics_dict_non_opt(y_test, y_pred, \"Neural Network\", nn)\n",
    "\n",
    "        plot_roc_curve(y_test, y_pred,\"Neural Network\")\n",
    "        plot_confusion_matrix(y_test,y_pred,\"Neural Network\")\n",
    "    else:\n",
    "        return nn\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuronal_network(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use xgboost to predict target variable\n",
    "def xgboost(X_train, X_test, y_train, y_test, hyperopt = False, parameters=dict()):\n",
    "    regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "\n",
    "    X_train.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_train.columns.values]\n",
    "    X.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X.columns.values]\n",
    "\n",
    "    # fit the model\n",
    "    xgb = XGBClassifier(random_state=42,**parameters)\n",
    "\n",
    "    if hyperopt == False:\n",
    "        xgb.fit(X_train, y_train)\n",
    "        # predict the target variable\n",
    "        y_pred = xgb.predict(X_test)\n",
    "        # print accuracy score and recall score\n",
    "        metrics(X,y,y_test, y_pred, crossval=True, modell=xgb)\n",
    "\n",
    "\n",
    "        metrics_dict_non_opt(y_test, y_pred, \"xGboost\", xgb)\n",
    "\n",
    "        plot_roc_curve(y_test, y_pred,\"xGboost\")\n",
    "        plot_confusion_matrix(y_test,y_pred,\"xGboost\")\n",
    "    else:\n",
    "        return xgb\n",
    "    return xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling Optimized with Hyperparametertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for hypertuned modells\n",
    "Opt_results = {\"Algo_name\": [], \"Recall\": [],\"Precision\": [],\"Accuracy\": [], \"F2\": [], \"F2-CV\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Hyperparamter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter tuning for logistic regression\n",
    "#tune hyperparameters\n",
    "parameters = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'penalty': ['l1', 'l2'],'intercept_scaling': [100, 500, 1000, 5000, 10000], 'class_weight': [\"balanced\", None]}\n",
    "# create grid search object with cross validation\n",
    "grid_search = GridSearchCV(logistic_regression(X_train, X_test, y_train, y_test, hyperopt=True), parameters, cv=cv, scoring=f2, n_jobs=-1)\n",
    "# fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "# print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "\n",
    "# save best hyperparameters in variable\n",
    "logreg_best_parameters = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression(X_train, X_test, y_train, y_test, hyperopt=False, parameters=logreg_best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versuch mit Optuna, Ergebnisse sind allerdings Ähnlich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# einmal mit optuna ausprobiert\n",
    "# import optuna\n",
    "import optuna \n",
    "# tune hyperparameters with optuna package\n",
    "def tune_logistic_regression(X_train, X_test, y_train, y_test):\n",
    "    # create optuna object\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    # define objective function\n",
    "    def objective(trial):\n",
    "        # define hyperparameters\n",
    "        C = trial.suggest_uniform(\"C\", 0.001, 1000)\n",
    "        penalty = trial.suggest_categorical(\"penalty\", [ \"l2\"])\n",
    "        intercept_scaling = trial.suggest_int(\"intercept_scaling\", 1, 100000)\n",
    "        class_weight = trial.suggest_categorical(\"class_weight\", [\"balanced\", None])\n",
    "        # create logistic regression object\n",
    "        logreg = LogisticRegression(C=C, penalty=penalty, intercept_scaling=intercept_scaling, class_weight=class_weight, random_state=42)\n",
    "        # fit the model\n",
    "        logreg.fit(X_train, y_train)\n",
    "        # predict the target variable\n",
    "        y_pred = logreg.predict(X_test)\n",
    "        # return recall score\n",
    "        return recall_score(y_test, y_pred)\n",
    "    # run optimization with objective function\n",
    "    study.optimize(objective, n_trials=100)\n",
    "    # print the best hyperparameters\n",
    "    print(\"Best hyperparameters:\", study.best_params)\n",
    "    print(\"Best score:\", study.best_value)\n",
    "    # save best hyperparameters in variable\n",
    "    logreg_best_parameters = study.best_params\n",
    "    # create logistic regression object with best hyperparameters\n",
    "    logreg = LogisticRegression(C=logreg_best_parameters[\"C\"], penalty=logreg_best_parameters[\"penalty\"], intercept_scaling=logreg_best_parameters[\"intercept_scaling\"], class_weight=logreg_best_parameters[\"class_weight\"], random_state=42)\n",
    "    # fit the model\n",
    "    logreg.fit(X_train, y_train)\n",
    "    # predict the target variable\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    # print accuracy score and recall score\n",
    "    metrics(X,y,y_test, y_pred, crossval=True, modell=logreg)\n",
    "\n",
    "tune_logistic_regression(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter tuning for random forest\n",
    "#tune hyperparameters\n",
    "parameters = {'n_estimators': [10, 100, 500], 'max_depth': [None, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20], 'max_features': [None, \"auto\", \"sqrt\", \"log2\"], 'min_samples_split': [2, 4, 6, 8, 10], 'min_samples_leaf': [1, 2, 4, 6, 8], 'bootstrap': [True, False]}\n",
    "# create grid search object with cross validation\n",
    "# count time for optimization\n",
    "start = time.time()\n",
    "grid_search = GridSearchCV(random_forest(X_train, X_test, y_train, y_test, hyperopt=True), parameters, cv=cv, scoring=f2 ,n_jobs= -1)\n",
    "# time for optimization\n",
    "end = time.time()\n",
    "print(\"Time for optimization:\", end - start)\n",
    "# fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "# print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "# save best hyperparameters in variable\n",
    "rf_best_parameters = grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass best hyperparameters to random forest function\n",
    "random_forest(X_train, X_test, y_train, y_test, hyperopt=False, parameters=rf_best_parameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xGboost Optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning for xgboost\n",
    "#tune hyperparameters\n",
    "parameters = {'n_estimators': [10, 100, 500], 'max_depth': [None, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20], 'max_features': [None, \"auto\", \"sqrt\", \"log2\"], 'min_samples_split': [2, 4, 6, 8, 10], 'min_samples_leaf': [1, 2, 4, 6, 8], 'bootstrap': [True, False]}\n",
    "# create grid search object with cross validation\n",
    "grid_search = GridSearchCV(xgboost(X_train, X_test, y_train, y_test, hyperopt=True), parameters, cv=cv, scoring=f2 ,n_jobs= -1)\n",
    "# fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "# print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "# save best hyperparameters in variable\n",
    "xgb_best_parameters = grid_search.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass best hyperparameters to xgboost function\n",
    "xgboost(X_train, X_test, y_train, y_test, hyperopt=False, parameters=xgb_best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuronal Network Optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter MLPClassifier with grid search\n",
    "#tune hyperparameters\n",
    "parameters = {'hidden_layer_sizes': [(15,)],'activation': ['logistic'],'solver': ['lbfgs', 'sgd', 'adam'], 'max_iter': [100], 'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0], 'learning_rate': ['constant', 'invscaling', 'adaptive'], 'learning_rate_init': [0.0001, 0.001, 0.01, 0.1], 'power_t': [0.5, 0.75, 0.9], 'shuffle': [True, False]}\n",
    " # create grid search object with cross validation\n",
    "grid_search = GridSearchCV(MLPClassifier(random_state=42), parameters, cv=cv, scoring=f2 ,n_jobs= -1)\n",
    "# fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "# print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "# save best hyperparameters in variable\n",
    "mlp_best_parameters = grid_search.best_params_\n",
    "# pass best hyperparameters to MLPClassifier function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Over/Undersampling SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import smote from imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create smote object\n",
    "smote = SMOTE(random_state=42,sampling_strategy=\"auto\", k_neighbors=10)\n",
    "# create smote train\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape before SMOTE:\", X_train.shape, y_train.shape)\n",
    "print(\"Shape after SMOTE:\", X_train_smote.shape, y_train_smote.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for logistic regression\n",
    "#tune hyperparameters\n",
    "parameters = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'penalty': [\"l2\"], 'intercept_scaling': [1, 10, 100, 1000], 'class_weight': [\"balanced\", None]}\n",
    "# create grid search object with cross validation\n",
    "grid_search = GridSearchCV(logistic_regression(X_train_smote, X_test, y_train_smote, y_test, hyperopt=True), parameters, cv=cv, scoring=f2 ,n_jobs= -1)\n",
    "# fit the model\n",
    "grid_search.fit(X_train_smote, y_train_smote)\n",
    "# print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "# save best hyperparameters in variable\n",
    "logreg_best_parameters_smote = grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression(X_train_smote, X_test, y_train_smote, y_test, hyperopt=False, parameters=logreg_best_parameters_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2_scores = []\n",
    "for train_index, test_index in cv.split(X,y):\n",
    "    X_train = X.iloc[train_index]\n",
    "    y_train = y.iloc[train_index]  \n",
    "    X_test = X.iloc[test_index]\n",
    "    y_test = y.iloc[test_index]  \n",
    "    sm = SMOTE(sampling_strategy=\"minority\",random_state=42, k_neighbors=5 , n_jobs=-1)\n",
    "    X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train, y_train)\n",
    "    model =  logistic_regression(X_train_oversampled, X_test, y_train_oversampled, y_test,hyperopt=True, **parameters)  # Choose a model here\n",
    "    model.fit(X_train_oversampled, y_train_oversampled)  \n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f'f-score: {fbeta_score(y_test, y_pred, beta = 2)}')\n",
    "    f2_scores.append(fbeta_score(y_test, y_pred, beta = 2))\n",
    "    \n",
    "print(f'Mean f-score: {statistics.mean(f2_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for random forest\n",
    "#tune hyperparameters\n",
    "parameters = {'n_estimators': [10, 100, 500], 'max_depth': [None, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20], 'max_features': [None, \"auto\", \"sqrt\", \"log2\"], 'min_samples_split': [2, 4, 6, 8, 10], 'min_samples_leaf': [1, 2, 4, 6, 8], 'bootstrap': [True, False]}\n",
    "# create grid search object with cross validation\n",
    "grid_search = GridSearchCV(random_forest(X_train_smote, X_test, y_train_smote, y_test, hyperopt=True), parameters, cv=cv, scoring=f2 ,n_jobs= -1)\n",
    "# fit the model\n",
    "grid_search.fit(X_train_smote, y_train_smote)\n",
    "# print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "# save best hyperparameters in variable\n",
    "rf_best_parameters_smote = grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest(X_train_smote, X_test, y_train_smote, y_test, hyperopt=False, parameters=rf_best_parameters_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2_scores = []\n",
    "for train_index, test_index in cv.split(X,y):\n",
    "    X_train = X.iloc[train_index]\n",
    "    y_train = y.iloc[train_index]  \n",
    "    X_test = X.iloc[test_index]\n",
    "    y_test = y.iloc[test_index]  \n",
    "    sm = SMOTE(sampling_strategy=\"minority\",random_state=42, k_neighbors=5 , n_jobs=-1)\n",
    "    X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train, y_train)\n",
    "    model =  random_forest(X_train_oversampled, X_test, y_train_oversampled, y_test,hyperopt=True, **parameters)  # Choose a model here\n",
    "    model.fit(X_train_oversampled, y_train_oversampled)  \n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f'f-score: {fbeta_score(y_test, y_pred, beta = 2)}')\n",
    "    f2_scores.append(fbeta_score(y_test, y_pred, beta = 2))\n",
    "    \n",
    "print(f'Mean f-score: {statistics.mean(f2_scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for logistic regression\n",
    "#tune hyperparameters\n",
    "parameters = {'n_estimators': [10, 100, 500], 'max_depth': [None, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20], 'max_features': [None, \"auto\", \"sqrt\", \"log2\"], 'min_samples_split': [2, 4, 6, 8, 10], 'min_samples_leaf': [1, 2, 4, 6, 8], 'bootstrap': [True, False]}\n",
    "\n",
    "# create grid search object with cross validation\n",
    "grid_search = GridSearchCV(xgboost(X_train_smote, X_test, y_train_smote, y_test, hyperopt=True), parameters, cv=cv, scoring=f2 ,n_jobs= -1)\n",
    "# fit the model\n",
    "grid_search.fit(X_train_smote, y_train_smote)\n",
    "# print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "# save best hyperparameters in variable\n",
    "xgb_best_parameters_smote = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2_scores = []\n",
    "for train_index, test_index in cv.split(X,y):\n",
    "    X_train = X.iloc[train_index]\n",
    "    y_train = y.iloc[train_index]  \n",
    "    X_test = X.iloc[test_index]\n",
    "    y_test = y.iloc[test_index]  \n",
    "    sm = SMOTE(sampling_strategy=\"minority\",random_state=42, k_neighbors=5 , n_jobs=-1)\n",
    "    X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train, y_train)\n",
    "    model =  xgboost(X_train_oversampled, X_test, y_train_oversampled, y_test,hyperopt=True, **parameters)  # Choose a model here\n",
    "    model.fit(X_train_oversampled, y_train_oversampled)  \n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f'f-score: {fbeta_score(y_test, y_pred, beta = 2)}')\n",
    "    f2_scores.append(fbeta_score(y_test, y_pred, beta = 2))\n",
    "    \n",
    "print(f'Mean f-score: {statistics.mean(f2_scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronale Netze SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exkurs Explainable AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wittgenstein as lw\n",
    "from wittgenstein.ripper import RIPPER\n",
    "from wittgenstein.interpret import interpret_model, score_fidelity\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data to pandas dataframe\n",
    "ripper_data = pd.read_csv(\"german_data_ripper.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# split features and target\n",
    "X = ripper_data.drop(['Target'], axis=1)\n",
    "y = ripper_data['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funktion for fitting Ripper model\n",
    "def ripper(X_train, X_test, y_train, y_test):\n",
    "    # create ripper object\n",
    "    ripper = lw.RIPPER(random_state=42,k=3, dl_allowance=64, n_discretize_bins=10, verbosity=0)\n",
    "    # fit the model\n",
    "    ripper.fit(X_train, y_train)\n",
    "    # predict the target variable\n",
    "    y_pred = ripper.predict(X_test)\n",
    "    # print accuracy score and recall score\n",
    "    metrics(X,y,y_test, y_pred, crossval=True, modell=ripper)\n",
    "\n",
    "    metrics_dict_non_opt(y_test, y_pred, \"Ripper\", ripper)\n",
    "\n",
    "   \n",
    "    ripper.out_model()\n",
    "\n",
    "    plot_roc_curve(y_test, y_pred,\"Ripper\")\n",
    "    plot_confusion_matrix(y_test,y_pred,\"Ripper\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ripper(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pyml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81c0c3b0c60f05152350124d7f4c066f9aabd4ebc0cecf0df048bc22ec2ec965"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
