{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json \n",
    "\n",
    "#plot confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import stratified k fold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, fbeta_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import re\n",
    "\n",
    "\n",
    "import statistics\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot roc curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "#plot roc curve\n",
    "def plot_roc_curve(y_test, y_pred,label):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic: '+label)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix function\n",
    "def plot_confusion_matrix(y_test, y_pred, label):\n",
    "    cm = confusion_matrix(y_test.tolist(), y_pred.tolist())\n",
    "    plt.figure()\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\")\n",
    "    plt.title('Confusion Matrix: '+label)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for metrics\n",
    "def metrics(X,y,y_test, y_pred, modell = None, crossval = True):\n",
    "\n",
    "\n",
    "    print(\"----- Metric for 80/20 Split -----\")\n",
    "    print(\"Accuracy score:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Recall score:\", recall_score(y_test, y_pred))\n",
    "    print(\"Precision score:\", precision_score(y_test, y_pred))\n",
    "    print(\"fbeta score:\", fbeta_score(y_test, y_pred, beta=2))\n",
    "    print(\"----- Metric for Cross Validation -----\")\n",
    "\n",
    "    if crossval == True:\n",
    "        modell.fit(X,y)\n",
    "        \n",
    "        print(\"Crossval score:\", statistics.mean(cross_val_score(modell ,X, y, scoring=f2,cv = cv )))\n",
    "        # print all the scores\n",
    "        print(\"Crossval score:\", cross_val_score(modell ,X, y, scoring=f2,cv = cv ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append metrics to dictionary\n",
    "def metrics_dict_non_opt(y_test, y_pred, label, modell = None):\n",
    "    Non_opt_results[\"Algo_name\"].append(label)\n",
    "    Non_opt_results[\"Recall\"].append(recall_score(y_test, y_pred))\n",
    "    Non_opt_results[\"Precision\"].append(precision_score(y_test, y_pred))\n",
    "    Non_opt_results[\"Accuracy\"].append(accuracy_score(y_test, y_pred))\n",
    "    Non_opt_results[\"F2\"].append(fbeta_score(y_test, y_pred,beta=2))\n",
    "    Non_opt_results[\"F2-CV\"].append(statistics.mean(cross_val_score(modell ,X, y, scoring=f2,cv = cv )))\n",
    "\n",
    "\n",
    "def metrics_dict_opt(y_test, y_pred, label, modell = None):\n",
    "    Opt_results[\"Algo_name\"].append(label)\n",
    "    Opt_results[\"Recall\"].append(recall_score(y_test, y_pred))\n",
    "    Opt_results[\"Precision\"].append(precision_score(y_test, y_pred))\n",
    "    Opt_results[\"Accuracy\"].append(accuracy_score(y_test, y_pred))\n",
    "    Opt_results[\"F2\"].append(fbeta_score(y_test, y_pred,beta=2))\n",
    "    Opt_results[\"F2-CV\"].append(statistics.mean(cross_val_score(modell ,X, y, scoring=f2,cv = cv )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2_measure(y_true, y_pred):\n",
    "\treturn fbeta_score(y_true, y_pred, beta=2)\n",
    "f2 = make_scorer(f2_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load german_data_cleaned.csv\n",
    "german_data = pd.read_csv('german_data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Feauture und Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split features and target\n",
    "X = german_data.drop(['Target'], axis=1)\n",
    "y = german_data['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Kreuzvalidierung\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary with algo names and score metrics for later analysis\n",
    "\n",
    "Non_opt_results = {\"Algo_name\": [], \"Recall\": [],\"Precision\": [],\"Accuracy\": [], \"F2\": [], \"F2-CV\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test set 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling Non-Optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use logistic regression to predict target variable\n",
    "# create logistic regression model with standard parameters\n",
    "\n",
    "def logistic_regression(X_train, X_test, y_train, y_test):\n",
    "    logreg = LogisticRegression(random_state=42)\n",
    "    # fit the model\n",
    "    logreg.fit(X_train, y_train)\n",
    "    # predict the target variable\n",
    "    y_pred = logreg.predict(X_test)\n",
    "\n",
    "    # print accuracy score and recall score\n",
    "    metrics(X,y,y_test, y_pred, crossval=True, modell=logreg)\n",
    "\n",
    "\n",
    "\n",
    "    # add algo name and recall score to dictionary\n",
    "    metrics_dict_non_opt(y_test, y_pred, \"Logistic Regression\", logreg)\n",
    "    plot_roc_curve(y_test, y_pred,\"Logistic Regression\")\n",
    "    plot_confusion_matrix(y_test,y_pred,\"Logistic Regression\")\n",
    "\n",
    "    return logreg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, X_test, y_train, y_test):\n",
    "    rf = RandomForestClassifier( random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    # predict the target variable\n",
    "    y_pred = rf.predict(X_test)\n",
    "    # print accuracy score and recall score\n",
    "    metrics(X,y,y_test, y_pred, crossval=True, modell=rf)\n",
    "\n",
    "\n",
    "    metrics_dict_non_opt(y_test, y_pred, \"Random forest\", rf)\n",
    "\n",
    "\n",
    "    # show feature importance for random forest\n",
    "\n",
    "    importances = rf.feature_importances_\n",
    "    # plot importances\n",
    "    plt.figure(figsize=(10,10))\n",
    "    # order importances\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    sns.barplot(x=importances[indices], y=X_train.columns, orient='h')\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.show()\n",
    "\n",
    "    plot_roc_curve(y_test, y_pred,\"Random Forest\")\n",
    "    plot_confusion_matrix(y_test,y_pred,\"Random Forest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuronal Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalerweise würde man die Daten für das Training mit einem Neuronalen Netz mithilfe eines scaler nomalisieren bzw. skalieren.\n",
    "Dies wurde auch mithilfe des minmax scalers und dem standardscaler versucht. Allerdings hatten beide einen negativen Einfluss auf die Ergebnisse des Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use neuro network to predict target variable\n",
    "# als baseline werden die Standardparameter verwendet\n",
    "# normalisieren\n",
    "# fit the model\n",
    "\n",
    "# import standard scaler from sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def neuronal_network(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "\n",
    "    nn = MLPClassifier(hidden_layer_sizes=(100), random_state=42)\n",
    "    nn.fit(X_train, y_train)\n",
    "    # predict the target variable\n",
    "    y_pred = nn.predict(X_test)\n",
    "    # print accuracy score and recall score\n",
    "    metrics(X,y,y_test, y_pred, crossval=True, modell=nn)\n",
    "\n",
    "\n",
    "    metrics_dict_non_opt(y_test, y_pred, \"Neural Network\", nn)\n",
    "\n",
    "    plot_roc_curve(y_test, y_pred,\"Neural Network\")\n",
    "    plot_confusion_matrix(y_test,y_pred,\"Neural Network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuronal_network(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use xgboost to predict target variable\n",
    "def xgboost(X_train, X_test, y_train, y_test):\n",
    "    regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "\n",
    "    X_train.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_train.columns.values]\n",
    "    X.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X.columns.values]\n",
    "\n",
    "    # fit the model\n",
    "    xgb = XGBClassifier(random_state=42)\n",
    "\n",
    "\n",
    "    xgb.fit(X_train, y_train)\n",
    "    # predict the target variable\n",
    "    y_pred = xgb.predict(X_test)\n",
    "    # print accuracy score and recall score\n",
    "    metrics(X,y,y_test, y_pred, crossval=True, modell=xgb)\n",
    "\n",
    "\n",
    "    metrics_dict_non_opt(y_test, y_pred, \"xGboost\", xgb)\n",
    "\n",
    "    plot_roc_curve(y_test, y_pred,\"xGboost\")\n",
    "    plot_confusion_matrix(y_test,y_pred,\"xGboost\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling Optimized with Hyperparametertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for hypertuned modells\n",
    "Opt_results = {\"Algo_name\": [], \"Recall\": [],\"Precision\": [],\"Accuracy\": [], \"F2\": [], \"F2-CV\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Hyperparamter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter tuning for logistic regression\n",
    "#tune hyperparameters\n",
    "parameters = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'penalty': ['l1', 'l2'],'intercept_scaling': [100, 500, 1000, 5000, 10000], 'class_weight': [\"balanced\", None]}\n",
    "# create grid search object with cross validation\n",
    "grid_search = GridSearchCV(logistic_regression(X_train, X_test, y_train, y_test), parameters, cv=cv, scoring=f2)\n",
    "# fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "# print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "\n",
    "# save best hyperparameters in variable\n",
    "logreg_best_parameters = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "import optuna \n",
    "# tune hyperparameters with optuna package\n",
    "def tune_logistic_regression(X_train, X_test, y_train, y_test):\n",
    "    # create optuna object\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    # define objective function\n",
    "    def objective(trial):\n",
    "        # define hyperparameters\n",
    "        C = trial.suggest_uniform(\"C\", 0.001, 1000)\n",
    "        penalty = trial.suggest_categorical(\"penalty\", [ \"l2\"])\n",
    "        intercept_scaling = trial.suggest_int(\"intercept_scaling\", 1, 100000)\n",
    "        class_weight = trial.suggest_categorical(\"class_weight\", [\"balanced\", None])\n",
    "        # create logistic regression object\n",
    "        logreg = LogisticRegression(C=C, penalty=penalty, intercept_scaling=intercept_scaling, class_weight=class_weight, random_state=42)\n",
    "        # fit the model\n",
    "        logreg.fit(X_train, y_train)\n",
    "        # predict the target variable\n",
    "        y_pred = logreg.predict(X_test)\n",
    "        # return recall score\n",
    "        return recall_score(y_test, y_pred)\n",
    "    # run optimization with objective function\n",
    "    study.optimize(objective, n_trials=100)\n",
    "    # print the best hyperparameters\n",
    "    print(\"Best hyperparameters:\", study.best_params)\n",
    "    print(\"Best score:\", study.best_value)\n",
    "    # save best hyperparameters in variable\n",
    "    logreg_best_parameters = study.best_params\n",
    "    # create logistic regression object with best hyperparameters\n",
    "    logreg = LogisticRegression(C=logreg_best_parameters[\"C\"], penalty=logreg_best_parameters[\"penalty\"], intercept_scaling=logreg_best_parameters[\"intercept_scaling\"], class_weight=logreg_best_parameters[\"class_weight\"], random_state=42)\n",
    "    # fit the model\n",
    "    logreg.fit(X_train, y_train)\n",
    "    # predict the target variable\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    # print accuracy score and recall score\n",
    "    metrics(X,y,y_test, y_pred, crossval=True, modell=logreg)\n",
    "\n",
    "tune_logistic_regression(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pyml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81c0c3b0c60f05152350124d7f4c066f9aabd4ebc0cecf0df048bc22ec2ec965"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
