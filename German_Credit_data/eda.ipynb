{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json \n",
    "\n",
    "#plot confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import stratified k fold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "import statistics\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas = pd.read_csv('../German_Credit_data/Data/german.data', sep=' ')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Da die Attributsnamen kodiert sind werden diese zu erst mithilfe der zugehörigen Dokumentation ersetzt. (Siehe Categorical.json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../German_Credit_data/Data/german.data\"\n",
    "\n",
    "names = ['Status Checking', 'Duration in Month', 'Credit History', 'Purpose', 'Credit Amount', \n",
    "         'Savings Account', 'Employement since', 'Installmentrate %', 'StatusSex', 'Otherdebtos', \n",
    "         'PresentResidence', 'Property', 'Age in years', 'Otherinstallment Plans', 'Housing', \n",
    "         'Number existing Credits', 'Job', 'Number people liable', 'Telephone', 'Foreign Worker', 'Target']\n",
    "german_data = pd.read_csv(file_path,names=names,delim_whitespace=True, header=None)\n",
    "catgories = open('../German_Credit_data/Data/categorical.json')\n",
    "json_data = json.load(catgories)\n",
    "json_data\n",
    "for h in names:\n",
    "    if h in json_data:\n",
    "        german_data[h] = german_data[h].map(json_data[h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistische Daten zur Datenanalyse\n",
    "german_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataframe_image as dfi\n",
    "#german_data.head(5).dfi.export('df.png')\n",
    "#df_styled = german_data.describe().style.background_gradient()\n",
    "# describe the data\n",
    "#dfi.export(df_styled, 'df_stats.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "german_data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prüfung der Daten auf Plausibilität und Allgemeine Datenanalyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check credit amount for outliers\n",
    "german_data['Credit Amount'].hist(bins=50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "german_data['Credit Amount'].plot(kind='hist', logx=False, bins=50)\n",
    "# plot histogram credit amout where target is 2\n",
    "german_data.loc[german_data['Target'] == 2, 'Credit Amount'].plot(kind='hist', logx=False, bins=50, color='red', figsize=(15, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot target variable as pie chart with target 1 and target 2 as labels colored red and blue\n",
    "\n",
    "german_data['Target'].value_counts().plot(kind='pie', colors=['blue', 'red'], autopct='%1.1f%%', figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zielvariable ist Ungleichverteilt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Untersuchung auf Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#box plot credit amount outliers\n",
    "german_data.boxplot(column='Credit Amount', by='Target', figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot credit amount\n",
    "german_data.plot(kind='scatter', x='Credit Amount', y='Target', figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of target variable with color 2 as red and color 1 as blue\n",
    "\n",
    "german_data['Target'].value_counts().plot(kind='bar', figsize=(15, 5), color=['blue', 'red'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate ZScore for credit amount --> for outlier detection\n",
    "german_data['ZScore'] = (german_data['Credit Amount'] - german_data['Credit Amount'].mean()) / german_data['Credit Amount'].std()\n",
    "#plot zscore for credit amount\n",
    "german_data['ZScore'].plot(kind='hist', bins=50, figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate IQR for credit amount\n",
    "def outlier_detection(df):\n",
    "    Q1 = np.percentile(german_data['Credit Amount'], 25,\n",
    "                    interpolation = 'midpoint')\n",
    "    \n",
    "    Q3 = np.percentile(german_data['Credit Amount'], 75,\n",
    "                    interpolation = 'midpoint')\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    upper = Q3 +1.5*IQR\n",
    "    lower = Q1 - 1.5*IQR\n",
    "    upper = german_data['Credit Amount'] >= (Q3+1.5*IQR)\n",
    "    \n",
    "    print(\"Upper bound:\",upper)\n",
    "    print(np.where(upper))\n",
    "    \n",
    "    # Below Lower bound\n",
    "    lower = german_data['Credit Amount'] <= (Q1-1.5*IQR)\n",
    "    print(\"Lower bound:\", lower)\n",
    "    print(np.where(lower))\n",
    "outlier_detection(german_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to determine outlier values\n",
    "def outlier_iqr(df):\n",
    "    quartile_1, quartile_3 = np.percentile(df, [25, 75])\n",
    "    iqr = quartile_3 - quartile_1\n",
    "    lower_bound = quartile_1 - (iqr * 1.5)\n",
    "    upper_bound = quartile_3 + (iqr * 1.5)\n",
    "    return np.where((df > upper_bound) | (df < lower_bound))\n",
    "# calculate the outlier values\n",
    "outlier_iqr(german_data['Credit Amount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datenanalyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform target value 2 to 1 and 1 to 0 to be binary\n",
    "# 0 = good credit ; 1 = bad credit\n",
    "german_data['Target'] = german_data['Target'].map({2:1, 1:0})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_data['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate correlation between target and feautures \n",
    "# only plot the 5 most correlated features with target variable\n",
    "\n",
    "def correlation_matrix(df):\n",
    "    corr_matrix = df.corr()\n",
    "    plt.figure(figsize=(25,25), dpi = 480)\n",
    "    sns.heatmap(corr_matrix, annot=True)\n",
    "    plt.show()\n",
    "\n",
    "    correlations = df.corr()['Target'].sort_values()\n",
    "\n",
    "    # Display correlations\n",
    "    print(' Positive Correlations:\\n', correlations)\n",
    "    print(' Negative Correlations:\\n', correlations)\n",
    "\n",
    "correlation_matrix(german_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through feautures and get datatype and check if they are categorical or numerical and store them into list for one hot encoding\n",
    "\n",
    "categorical_features = []\n",
    "numerical_features = []\n",
    "for i in german_data.columns:\n",
    "    print(i, german_data[i].dtype)\n",
    "    if german_data[i].dtype == 'object':\n",
    "        print(i, german_data[i].dtype)\n",
    "        categorical_features.append(i)\n",
    "    else:\n",
    "        print(i, german_data[i].dtype)\n",
    "        numerical_features.append(i)\n",
    "\n",
    "\n",
    "print(categorical_features, \"Numerical:\" ,numerical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stark Korrelation zwischen Credit Amount und Duration in Month --> Macht Sinn, da größere Kredite in der Regel längere Laufzeiten haben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairplot um einen überblick über die Verteilung zu bekommen\n",
    "sns.pairplot(german_data, hue='Target', size=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3, figsize=(15, 15))\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "\n",
    "for category in numerical_features:\n",
    "    sns.boxplot(y=german_data[category], x=german_data[\"Target\"].replace({0:\"good\", 1:\"bad\"}), ax=axs[i, j], orient=\"v\", showmeans=True)\n",
    "    j += 1\n",
    "    if j%3 == 0:\n",
    "        j = 0\n",
    "        i += 1\n",
    "        \n",
    "axs[2, 1].set_visible(True)\n",
    "fig.delaxes(axs[2, 1])\n",
    "axs[2, 2].set_visible(True)\n",
    "fig.delaxes(axs[2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functio to plot plotbar for categorical features \n",
    "def plot_bar(df, feature):\n",
    "    sns.countplot(x=feature, data=df, hue='Target')\n",
    "    plt.show()\n",
    "\n",
    "plot_bar(german_data, 'Purpose')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40, 15))\n",
    "for i in range(0, len(categorical_features)):\n",
    "    plt.subplot(5, 3, i+1)\n",
    "    sns.countplot(x = german_data[categorical_features[i]], orient='v', hue=german_data['Target'])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(10,6))\n",
    "ax = sns.distplot(german_data['Duration in Month'], hist=True, kde=False, \n",
    "             bins=72, color = '#bd7a51', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4})\n",
    "ax.set_ylabel('# of Customers')\n",
    "ax.set_xlabel('Duration in month')\n",
    "ax.set_title('# of Customers by their duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Housing = german_data.groupby('Housing')['Housing'].count()\n",
    "data_Savingaccounts = german_data.groupby('Savings Account')['Savings Account'].count()\n",
    "data_Checkingaccount = german_data.groupby('Status Checking')['Status Checking'].count()\n",
    "data_Purpose = german_data.groupby('Purpose')['Purpose'].count()\n",
    "\n",
    "data_Housing = pd.DataFrame({'Housing':data_Housing.index, 'Count':data_Housing.values})\n",
    "data_Savingaccounts = pd.DataFrame({'Savings Account':data_Savingaccounts.index, 'Count':data_Savingaccounts.values})\n",
    "data_Checkingaccount = pd.DataFrame({'Status Checking':data_Checkingaccount.index, 'Count':data_Checkingaccount.values})\n",
    "data_Purpose = pd.DataFrame({'Purpose':data_Purpose.index, 'Count':data_Purpose.values})\n",
    "\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "\n",
    "ax1 = plt.subplot2grid((2,2),(0,0))\n",
    "plt.pie(data_Housing['Count'],labels = data_Housing['Housing'],autopct='%1.1f%%');\n",
    "plt.title('Housing split in data');\n",
    "\n",
    "ax1 = plt.subplot2grid((2,2),(0,1))\n",
    "plt.pie(data_Savingaccounts['Count'],labels = data_Savingaccounts['Savings Account'],autopct='%1.1f%%');\n",
    "plt.title('Saving accounts Split in data');\n",
    "\n",
    "ax1 = plt.subplot2grid((2,2),(1,0))\n",
    "plt.pie(data_Checkingaccount['Count'],labels = data_Checkingaccount['Status Checking'],autopct='%1.1f%%');\n",
    "plt.title('Status Checking Split in data');\n",
    "\n",
    "ax1 = plt.subplot2grid((2,2),(1,1))\n",
    "plt.pie(data_Purpose['Count'],labels = data_Purpose['Purpose'],autopct='%1.1f%%');\n",
    "plt.title('Purpose Split in data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feauture Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since there is some correlation between credit amount and  duration in months we create a new feature\n",
    "# credit amount divided by duration in months \n",
    "german_data['Credit Amount per Month'] = german_data['Credit Amount'] / german_data['Duration in Month']\n",
    "\n",
    "# drop the duration in month feature and credit amount feature\n",
    "\n",
    "german_data.drop(['Duration in Month', 'Credit Amount'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split column StatusSex by delimiter \":\"\n",
    "#german_data['StatusSex'] = german_data['StatusSex'].str.split(\":\", n=-1, expand=False)\n",
    "# create new column for Status\n",
    "#german_data['Sex'] = german_data['StatusSex'].str.get(0)\n",
    "#german_data['Status'] = german_data['StatusSex'].str.get(1)\n",
    "\n",
    "# drop column StatusSex\n",
    "\n",
    "#german_data = german_data.drop(columns=\"StatusSex\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through feautures and get datatype and check if they are categorical or numerical and store them into list for one hot encoding\n",
    "\n",
    "categorical_features = []\n",
    "numerical_features = []\n",
    "for i in german_data.columns:\n",
    "    print(i, german_data[i].dtype)\n",
    "    if german_data[i].dtype == 'object':\n",
    "        print(i, german_data[i].dtype)\n",
    "        categorical_features.append(i)\n",
    "    else:\n",
    "        print(i, german_data[i].dtype)\n",
    "        numerical_features.append(i)\n",
    "\n",
    "\n",
    "print(categorical_features, \"Numerical:\" ,numerical_features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode categorical features\n",
    "german_data = pd.get_dummies(german_data, columns=categorical_features)\n",
    "\n",
    "german_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# still 1000 rows but 61 columns after one hot encoding\n",
    "german_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation after one hot encoding\n",
    "\n",
    "corr = correlation_matrix(german_data)\n",
    "\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop zscore\n",
    "german_data = german_data.drop(['ZScore'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot roc curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "#plot roc curve\n",
    "def plot_roc_curve(y_test, y_pred,label):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic: '+label)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix function\n",
    "def plot_confusion_matrix(y_test, y_pred, label):\n",
    "    cm = confusion_matrix(y_test.tolist(), y_pred.tolist())\n",
    "    plt.figure()\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\")\n",
    "    plt.title('Confusion Matrix: '+label)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for metrics\n",
    "def metrics(y_test, y_pred):\n",
    "    print(\"Accuracy score:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Recall score:\", recall_score(y_test, y_pred))\n",
    "    print(\"Precision score:\", precision_score(y_test, y_pred))\n",
    "    print(\"fbeta score:\", fbeta_score(y_test, y_pred, beta=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append metrics to dictionary\n",
    "def metrics_dict_non_opt(y_test, y_pred, label):\n",
    "    Non_opt_results[\"Algo_name\"].append(label)\n",
    "    Non_opt_results[\"Recall\"].append(recall_score(y_test, y_pred))\n",
    "    Non_opt_results[\"Precision\"].append(precision_score(y_test, y_pred))\n",
    "    Non_opt_results[\"Accuracy\"].append(accuracy_score(y_test, y_pred))\n",
    "    Non_opt_results[\"F2\"].append(fbeta_score(y_test, y_pred,beta=2))\n",
    "\n",
    "def metrics_dict_opt(y_test, y_pred, label):\n",
    "    Opt_results[\"Algo_name\"].append(label)\n",
    "    Opt_results[\"Recall\"].append(recall_score(y_test, y_pred))\n",
    "    Opt_results[\"Precision\"].append(precision_score(y_test, y_pred))\n",
    "    Opt_results[\"Accuracy\"].append(accuracy_score(y_test, y_pred))\n",
    "    Opt_results[\"F2\"].append(fbeta_score(y_test, y_pred,beta=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for adding results to dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Feauture und Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split features and target\n",
    "X = german_data.drop(['Target'], axis=1)\n",
    "y = german_data['Target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Modelling with Logistic Regression as Baseline, xGboost and a neuronal Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary with algo names and score metrics for later analysis\n",
    "\n",
    "Non_opt_results = {\"Algo_name\": [], \"Recall\": [],\"Precision\": [],\"Accuracy\": [], \"F2\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.765\n",
      "Recall score: 0.5423728813559322\n",
      "Precision score: 0.6153846153846154\n",
      "fbeta score: 0.5555555555555556\n",
      "Crossval score: 0.47368421052631576\n",
      "Crossval score: [0.6        0.4        0.42105263]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hornd\\AppData\\Local\\R-MINI~1\\envs\\pyml\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hornd\\AppData\\Local\\R-MINI~1\\envs\\pyml\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hornd\\AppData\\Local\\R-MINI~1\\envs\\pyml\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hornd\\AppData\\Local\\R-MINI~1\\envs\\pyml\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hornd\\AppData\\Local\\R-MINI~1\\envs\\pyml\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hornd\\AppData\\Local\\R-MINI~1\\envs\\pyml\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\hornd\\AppData\\Local\\R-MINI~1\\envs\\pyml\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# use logistic regression to predict target variable\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, fbeta_score\n",
    "\n",
    "# split data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(german_data.drop('Target', axis=1), german_data['Target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# create logistic regression model with standard parameters\n",
    "\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "# fit the model\n",
    "logreg.fit(X_train, y_train)\n",
    "# predict the target variable\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# print accuracy score and recall score\n",
    "metrics(y_test, y_pred)\n",
    "\n",
    "print(\"Crossval score:\", statistics.mean(cross_val_score(logreg ,X_test, y_test, scoring='recall',cv = 3 )))\n",
    "print(\"Crossval score:\", cross_val_score(logreg ,X_test, y_test, scoring='recall',cv = 3 ))\n",
    "\n",
    "# add algo name and recall score to dictionary\n",
    "metrics_dict_non_opt(y_test, y_pred, \"Logistic Regression\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(y_test, y_pred,\"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test,y_pred,\"Logistic Regression\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.78\n",
      "Recall score: 0.4067796610169492\n",
      "Precision score: 0.7272727272727273\n",
      "fbeta score: 0.446096654275093\n"
     ]
    }
   ],
   "source": [
    "# use random forest to predict target variable\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# fit the model\n",
    "rf = RandomForestClassifier( random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "# predict the target variable\n",
    "y_pred = rf.predict(X_test)\n",
    "# print accuracy score and recall score\n",
    "metrics(y_test, y_pred)\n",
    "\n",
    "metrics_dict_non_opt(y_test, y_pred, \"Random forest\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(y_test, y_pred,\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test,y_pred,\"Random Forest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use neuro network to predict target variable\n",
    "# als baseline werden die Standardparameter verwendet\n",
    "# normalisieren\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# fit the model\n",
    "nn = MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, random_state=42, activation=\"logistic\", alpha=0.0115)\n",
    "nn.fit(X_train, y_train)\n",
    "# predict the target variable\n",
    "y_pred = nn.predict(X_test)\n",
    "# print accuracy score and recall score\n",
    "metrics(y_test, y_pred)\n",
    "\n",
    "metrics_dict_non_opt(y_test, y_pred, \"Neural Network\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(y_test, y_pred,\"Neural Network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test,y_pred,\"Neural Network\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use xgboost to predict target variable\n",
    "from xgboost import XGBClassifier\n",
    "import re\n",
    "\n",
    "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "\n",
    "X_train.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_train.columns.values]\n",
    "\n",
    "# fit the model\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "# predict the target variable\n",
    "y_pred = xgb.predict(X_test)\n",
    "# print accuracy score and recall score\n",
    "metrics(y_test, y_pred)\n",
    "\n",
    "metrics_dict_non_opt(y_test, y_pred, \"xGboost\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(y_test, y_pred,\"xGboost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "plot_confusion_matrix(\"xGboost\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ergebnisse nicht Optimierte Modelle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Non_opt_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modells Optimized with Hyperparametertuning\n",
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for hypertuned modells\n",
    "Opt_results = {\"Algo_name\": [], \"Recall\": [],\"Precision\": [],\"Accuracy\": [], \"F2\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter tuning for logistic regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#tune hyperparameters\n",
    "parameters = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'penalty': ['l1', 'l2'],'intercept_scaling': [100, 500, 1000, 5000, 10000]}\n",
    "# create grid search object with cross validation\n",
    "grid_search = GridSearchCV(logreg, parameters, cv=5, scoring='recall')\n",
    "# fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "# print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model with the best hyperparameters\n",
    "'''\n",
    "solver to liblinear --> better score; standard lbfgs\n",
    "'''\n",
    "logreg = LogisticRegression(C=grid_search.best_params_['C'], penalty=grid_search.best_params_['penalty'], intercept_scaling=grid_search.best_params_['intercept_scaling'], solver = 'liblinear' )\n",
    "# fit the model\n",
    "logreg.fit(X_train, y_train)\n",
    "# predict the target variable\n",
    "y_pred = logreg.predict(X_test)\n",
    "# print accuracy score and recall score\n",
    "metrics(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict_opt(y_test, y_pred, \"Logistic regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search parameters for random forest\n",
    "parameters = {'n_estimators': [10, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], 'max_features': ['auto', 'sqrt', 'log2'],'max_depth': [None, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20]}\n",
    "# create grid search object with cross validation\n",
    "grid_search = GridSearchCV(rf, parameters, cv=5, scoring='recall')\n",
    "# fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "# print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model with the best hyperparameters\n",
    "rf = RandomForestClassifier(n_estimators=grid_search.best_params_['n_estimators'], max_features=grid_search.best_params_['max_features'], max_depth=grid_search.best_params_['max_depth'])\n",
    "# fit the model\n",
    "rf.fit(X_train, y_train)\n",
    "# predict the target variable\n",
    "y_pred = rf.predict(X_test)\n",
    "# print accuracy score and recall score\n",
    "metrics(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the results to the dictionary\n",
    "metrics_dict_opt(y_test, y_pred, \"Random Forest\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune hyperparameters for xgboost\n",
    "parameters = {'n_estimators': [10, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], 'max_depth': [2, 4, 6, 8, 10, 12, 14, 16, 18, 20], 'learning_rate': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]}\n",
    "# create grid search object with cross validation\n",
    "grid_search = GridSearchCV(xgb, parameters, cv=5, scoring='recall')\n",
    "# fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "# print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model with the best hyperparameters\n",
    "xgb = XGBClassifier(n_estimators=grid_search.best_params_['n_estimators'], max_depth=grid_search.best_params_['max_depth'], learning_rate=grid_search.best_params_['learning_rate'])\n",
    "# fit the model\n",
    "xgb.fit(X_train, y_train)\n",
    "# predict the target variable\n",
    "y_pred = xgb.predict(X_test)\n",
    "# print accuracy score and recall score\n",
    "\n",
    "metrics(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(y_test, y_pred, \"xGboost Opt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the results to the dictionary\n",
    "metrics_dict_opt(y_test, y_pred, \"xGboost\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronal Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune hyperparameters for neural network\n",
    "parameters = {'hidden_layer_sizes': [(15,)],'activation': ['logistic', 'relu', 'tanh'],'solver': ['lbfgs', 'sgd', 'adam'], 'max_iter': [100], 'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0], 'learning_rate': ['constant', 'invscaling', 'adaptive'], 'learning_rate_init': [0.0001, 0.001, 0.01, 0.1, 1.0], 'power_t': [0.5, 0.75, 0.9], 'shuffle': [True, False]}\n",
    "# create grid search object with cross validation\n",
    "grid_search = GridSearchCV(nn, parameters, cv=5, scoring='recall')\n",
    "# fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "# print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model with the best hyperparameters\n",
    "nn = MLPClassifier(hidden_layer_sizes=grid_search.best_params_['hidden_layer_sizes'], activation=grid_search.best_params_['activation'], solver=grid_search.best_params_['solver'], max_iter=grid_search.best_params_['max_iter'], alpha=grid_search.best_params_['alpha'], learning_rate=grid_search.best_params_['learning_rate'], learning_rate_init=grid_search.best_params_['learning_rate_init'], power_t=grid_search.best_params_['power_t'], shuffle=grid_search.best_params_['shuffle'])\n",
    "# fit the model\n",
    "nn.fit(X_train, y_train)\n",
    "# predict the target variable\n",
    "y_pred = nn.predict(X_test)\n",
    "# print accuracy score and recall score\n",
    "metrics(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot roc curve\n",
    "\n",
    "plot_roc_curve(y_test, y_pred, \"NN Opt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict_opt(y_test, y_pred, \"Neuronal Network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opt_results to excel file\n",
    "Opt_results = pd.DataFrame(Opt_results)\n",
    "Opt_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non_opt_results to excel file\n",
    "Non_opt_results = pd.DataFrame(Non_opt_results)\n",
    "Non_opt_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export nonopt to csv\n",
    "Non_opt_results.to_csv(\"Non_opt_results.csv\", index=False)\n",
    "Opt_results.to_csv(\"Non_opt_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperopt Hyperparametertuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kreuzvalidierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stratified kfold object\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "# create empty list to store recall scores\n",
    "recall_scores = []\n",
    "# perform stratified kfold cross validation\n",
    "for train, test in kfold.split(X_train, y_train):\n",
    "    # create model\n",
    "    logreg = LogisticRegression(C=grid_search.best_params_['C'], penalty=grid_search.best_params_['penalty'], intercept_scaling=grid_search.best_params_['intercept_scaling'], solver = 'liblinear' )\n",
    "    # fit model\n",
    "    logreg.fit(X_train.iloc[train], y_train.iloc[train])\n",
    "    # predict on test set\n",
    "    y_pred = logreg.predict(X_train.iloc[test])\n",
    "    # append recall score to list\n",
    "    recall_scores.append(recall_score(y_train.iloc[test], y_pred))\n",
    "    # print recall score\n",
    "    print(\"Recall score:\", recall_score(y_train.iloc[test], y_pred))\n",
    "# print mean recall score\n",
    "print(\"Mean recall score:\", statistics.mean(recall_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Running 10-Fold Cross validation on a given algorithmd\n",
    "# Passing full data X and y because the K-fold will split the data and automatically choose train/test\n",
    "Accuracy_Values=cross_val_score(logreg, X , y, cv=10, scoring='recall')\n",
    "print('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\n",
    "print('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampling mit Smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling with smote\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas\n",
    "- check label encoding vs one hor encoding\n",
    "- hyperparamter tuning\n",
    "- use shape for neuronal network\n",
    "- feauture importance\n",
    "\n",
    "- Feauture engineering?\n",
    "- oversampling?\n",
    "\n",
    "\n",
    "https://optuna.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81c0c3b0c60f05152350124d7f4c066f9aabd4ebc0cecf0df048bc22ec2ec965"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pyml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
