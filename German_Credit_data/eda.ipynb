{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json \n",
    "\n",
    "#plot confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas = pd.read_csv('../German_Credit_data/Data/german.data', sep=' ')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Da die Attributsnamen kodiert sind werden diese zu erst mithilfe der zugehörigen Dokumentation ersetzt. (Siehe Categorical.json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../German_Credit_data/Data/german.data\"\n",
    "\n",
    "names = ['Status Checking', 'Duration in Month', 'Credit History', 'Purpose', 'Credit Amount', \n",
    "         'Savings Account', 'Employement since', 'Installmentrate %', 'StatusSex', 'Otherdebtos', \n",
    "         'PresentResidence', 'Property', 'Age in years', 'Otherinstallment Plans', 'Housing', \n",
    "         'Number existing Credits', 'Job', 'Number people liable', 'Telephone', 'Foreign Worker', 'Target']\n",
    "german_data = pd.read_csv(file_path,names=names,delim_whitespace=True, header=None)\n",
    "catgories = open('../German_Credit_data/Data/categorical.json')\n",
    "json_data = json.load(catgories)\n",
    "json_data\n",
    "for h in names:\n",
    "    if h in json_data:\n",
    "        german_data[h] = german_data[h].map(json_data[h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataframe_image as dfi\n",
    "#german_data.head(5).dfi.export('df.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "german_data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_styled = german_data.describe().style.background_gradient()\n",
    "# describe the data\n",
    "#dfi.export(df_styled, 'df_stats.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prüfung der Daten auf Plausibilität und Allgemeine Datenanalyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check credit amount for outliers\n",
    "german_data['Credit Amount'].hist(bins=50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "german_data['Credit Amount'].plot(kind='hist', logx=False, bins=50)\n",
    "# plot histogram credit amout where target is 2\n",
    "german_data.loc[german_data['Target'] == 2, 'Credit Amount'].plot(kind='hist', logx=False, bins=50, color='red', figsize=(15, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot target variable as pie chart with target 1 and target 2 as labels colored red and blue\n",
    "\n",
    "german_data['Target'].value_counts().plot(kind='pie', colors=['red', 'blue'], autopct='%1.1f%%', figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zielvariable ist Ungleichverteilt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Untersuchung auf Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#box plot credit amount outliers\n",
    "german_data.boxplot(column='Credit Amount', by='Target', figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot credit amount\n",
    "german_data.plot(kind='scatter', x='Credit Amount', y='Target', figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of target variable with color 2 as red and color 1 as blue\n",
    "\n",
    "german_data['Target'].value_counts().plot(kind='bar', figsize=(15, 5), color=['blue', 'red'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate ZScore for credit amount --> for outlier detection\n",
    "german_data['ZScore'] = (german_data['Credit Amount'] - german_data['Credit Amount'].mean()) / german_data['Credit Amount'].std()\n",
    "#plot zscore for credit amount\n",
    "german_data['ZScore'].plot(kind='hist', bins=50, figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate IQR for credit amount\n",
    "def outlier_detection(df):\n",
    "    Q1 = np.percentile(german_data['Credit Amount'], 25,\n",
    "                    interpolation = 'midpoint')\n",
    "    \n",
    "    Q3 = np.percentile(german_data['Credit Amount'], 75,\n",
    "                    interpolation = 'midpoint')\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    upper = Q3 +1.5*IQR\n",
    "    lower = Q1 - 1.5*IQR\n",
    "    upper = german_data['Credit Amount'] >= (Q3+1.5*IQR)\n",
    "    \n",
    "    print(\"Upper bound:\",upper)\n",
    "    print(np.where(upper))\n",
    "    \n",
    "    # Below Lower bound\n",
    "    lower = german_data['Credit Amount'] <= (Q1-1.5*IQR)\n",
    "    print(\"Lower bound:\", lower)\n",
    "    print(np.where(lower))\n",
    "outlier_detection(german_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to determine outlier values\n",
    "def outlier_iqr(df):\n",
    "    quartile_1, quartile_3 = np.percentile(df, [25, 75])\n",
    "    iqr = quartile_3 - quartile_1\n",
    "    lower_bound = quartile_1 - (iqr * 1.5)\n",
    "    upper_bound = quartile_3 + (iqr * 1.5)\n",
    "    return np.where((df > upper_bound) | (df < lower_bound))\n",
    "# calculate the outlier values\n",
    "outlier_iqr(german_data['Credit Amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform target value 2 to 1 and 1 to 0 to be binary\n",
    "# 0 = good credit ; 1 = bad credit\n",
    "german_data['Target'] = german_data['Target'].map({2:1, 1:0})\n",
    "\n",
    "# map value 0 to string \"good\" credit and 1 to \"bad\" credit\n",
    "#german_data['Target'] = german_data['Target'].map({0:'good', 1:'bad'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_data['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate correlation between target and feautures \n",
    "# only plot the 5 most correlated features with target variable\n",
    "\n",
    "def correlation_matrix(df):\n",
    "    corr_matrix = df.corr()\n",
    "    plt.figure(figsize=(36,36), dpi = 480)\n",
    "    sns.heatmap(corr_matrix, annot=True)\n",
    "    plt.show()\n",
    "\n",
    "    correlations = df.corr()['Target'].sort_values()\n",
    "\n",
    "    # Display correlations\n",
    "    print(' Positive Correlations:\\n', correlations)\n",
    "    print(' Negative Correlations:\\n', correlations)\n",
    "\n",
    "correlation_matrix(german_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through feautures and get datatype and check if they are categorical or numerical and store them into list for one hot encoding\n",
    "categorical_features = []\n",
    "numerical_features = []\n",
    "for i in german_data.columns:\n",
    "    print(i, german_data[i].dtype)\n",
    "    if german_data[i].dtype == 'object':\n",
    "        print(i, german_data[i].dtype)\n",
    "        categorical_features.append(i)\n",
    "    else:\n",
    "        print(i, german_data[i].dtype)\n",
    "        numerical_features.append(i)\n",
    "\n",
    "\n",
    "print(categorical_features, \"Numerical:\" ,numerical_features)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode categorical features\n",
    "german_data = pd.get_dummies(german_data, columns=categorical_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# still 1000 rows but 63 columns after one hot encoding\n",
    "german_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop zscore\n",
    "german_data = german_data.drop(['ZScore'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Modelling with Logistic Regression as Baseline, xGboost and a neuronal Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary with algo names and score metrics for later analysis\n",
    "\n",
    "Non_opt_results = {\"Algo_name\": [], \"Recall\": [],\"Precision\": [],\"Accuracy\": [], \"F2\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use logistic regression to predict target variable\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, fbeta_score\n",
    "\n",
    "# split data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(german_data.drop('Target', axis=1), german_data['Target'], test_size=0.15, random_state=42)\n",
    "\n",
    "# create logistic regression model\n",
    "'''\n",
    "solver to liblinear --> better score; standard lbfgs\n",
    "'''\n",
    "logreg = LogisticRegression(max_iter=100, intercept_scaling=100, solver = 'liblinear')\n",
    "# fit the model\n",
    "logreg.fit(X_train, y_train)\n",
    "# predict the target variable\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# print accuracy score and recall score\n",
    "print(\"Accuracy score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Recall score:\", recall_score(y_test, y_pred))\n",
    "print(\"Precision score:\", precision_score(y_test, y_pred))\n",
    "print(\"fbeta score:\", fbeta_score(y_test, y_pred, beta=2))\n",
    "\n",
    "# add algo name and recall score to dictionary\n",
    "Non_opt_results[\"Algo_name\"].append(\"Logistic Regression\")\n",
    "Non_opt_results[\"Recall\"].append(recall_score(y_test, y_pred))\n",
    "Non_opt_results[\"Precision\"].append(precision_score(y_test, y_pred))\n",
    "Non_opt_results[\"Accuracy\"].append(accuracy_score(y_test, y_pred))\n",
    "Non_opt_results[\"F2\"].append(fbeta_score(y_test, y_pred,beta=2))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot roc curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "#plot roc curve\n",
    "def plot_roc_curve(y_test, y_pred,label):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic: '+label)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "# call the function roc curve\n",
    "plot_roc_curve(y_test, y_pred,\"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "def plot_confusion_matrix(label):\n",
    "    #plot confusion matrix with labels\n",
    "    plt.figure()\n",
    "    sns.heatmap(confusion_matrix, annot=True, fmt='d')\n",
    "    plt.title('Confusion matrix:'+ label)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# plot confusion matrix\n",
    "plot_confusion_matrix(\"Logistic Regression\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use random forest to predict target variable\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# fit the model\n",
    "rf = RandomForestClassifier( random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "# predict the target variable\n",
    "y_pred = rf.predict(X_test)\n",
    "# print accuracy score and recall score\n",
    "print(\"Accuracy score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Recall score:\", recall_score(y_test, y_pred))\n",
    "print(\"Precision score:\", precision_score(y_test, y_pred))\n",
    "print(\"fbeta score:\", fbeta_score(y_test, y_pred, beta=2))\n",
    "\n",
    "Non_opt_results[\"Algo_name\"].append(\"Random Forest\")\n",
    "Non_opt_results[\"Recall\"].append(recall_score(y_test, y_pred))\n",
    "Non_opt_results[\"Precision\"].append(precision_score(y_test, y_pred))\n",
    "Non_opt_results[\"Accuracy\"].append(accuracy_score(y_test, y_pred))\n",
    "Non_opt_results[\"F2\"].append(fbeta_score(y_test, y_pred,beta=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(y_test, y_pred,\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use neuro network to predict target variable\n",
    "# als baseline werden die Standardparameter verwendet\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# fit the model\n",
    "nn = MLPClassifier(hidden_layer_sizes=(23,), max_iter=1000, random_state=42)\n",
    "nn.fit(X_train, y_train)\n",
    "# predict the target variable\n",
    "y_pred = nn.predict(X_test)\n",
    "# print accuracy score and recall score\n",
    "print(\"Accuracy score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Recall score:\", recall_score(y_test, y_pred))\n",
    "print(\"Precision score:\", precision_score(y_test, y_pred))\n",
    "print(\"fbeta score:\", fbeta_score(y_test, y_pred, beta=2))\n",
    "\n",
    "Non_opt_results[\"Algo_name\"].append(\"Neural Network\")\n",
    "Non_opt_results[\"Recall\"].append(recall_score(y_test, y_pred))\n",
    "Non_opt_results[\"Precision\"].append(precision_score(y_test, y_pred))\n",
    "Non_opt_results[\"Accuracy\"].append(accuracy_score(y_test, y_pred))\n",
    "Non_opt_results[\"F2\"].append(fbeta_score(y_test, y_pred,beta=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(y_test, y_pred,\"Neural Network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\"Neural Network\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use xgboost to predict target variable\n",
    "from xgboost import XGBClassifier\n",
    "import re\n",
    "\n",
    "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "\n",
    "X_train.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_train.columns.values]\n",
    "\n",
    "# fit the model\n",
    "xgb = XGBClassifier(n_estimators=1000, max_depth=1000, random_state=42)\n",
    "\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "# predict the target variable\n",
    "y_pred = xgb.predict(X_test)\n",
    "# print accuracy score and recall score\n",
    "print(\"Accuracy score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Recall score:\", recall_score(y_test, y_pred))\n",
    "print(\"Precision score:\", precision_score(y_test, y_pred))\n",
    "print(\"fbeta score:\", fbeta_score(y_test, y_pred, beta=2))\n",
    "\n",
    "Non_opt_results[\"Algo_name\"].append(\"xGboost\")\n",
    "Non_opt_results[\"Recall\"].append(recall_score(y_test, y_pred))\n",
    "Non_opt_results[\"Precision\"].append(precision_score(y_test, y_pred))\n",
    "Non_opt_results[\"Accuracy\"].append(accuracy_score(y_test, y_pred))\n",
    "Non_opt_results[\"F2\"].append(fbeta_score(y_test, y_pred,beta=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(y_test, y_pred,\"xGboost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\"xGboost\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ergebnisse nicht Optimierte Modelle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Non_opt_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modells Optimized with Hyperparametertuning\n",
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for hypertuned modells\n",
    "Opt_results = {\"Algo_name\": [], \"Recall\": [],\"Accuracy\": [], \"F2\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter tuning for logistic regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#tune hyperparameters\n",
    "parameters = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'penalty': ['l1', 'l2'],'intercept_scaling': [100, 500, 1000, 5000, 10000]}\n",
    "# create grid search object with cross validation\n",
    "grid_search = GridSearchCV(logreg, parameters, cv=5, scoring='recall')\n",
    "# fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "# print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tune hyperparameters for random forest\n",
    "parameters = {'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], 'max_depth': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]}\n",
    "# create grid search object with cross validation\n",
    "grid_search = GridSearchCV(rf, parameters, cv=5, scoring='recall')\n",
    "# fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "# print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non_opt_results to excel file\n",
    "Non_opt_results = pd.DataFrame(Non_opt_results)\n",
    "Non_opt_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperopt Hyperparametertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hyperopt for hyperparameter tuning\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "# define the objective function\n",
    "def objective(params):\n",
    "    # create a logistic regression object\n",
    "    logreg = LogisticRegression(C=params['C'], penalty=params['penalty'], intercept_scaling=params['intercept_scaling'])\n",
    "    # fit the model\n",
    "    logreg.fit(X_train, y_train)\n",
    "    # predict the target variable\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    # return the accuracy score\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "# define the hyperparameter space\n",
    "space = {'C': hp.uniform('C', 0.0001, 1000), 'penalty': hp.choice('penalty', ['l2']), 'intercept_scaling': hp.uniform('intercept_scaling', 0.0001, 10000)}\n",
    "# create a Trials object\n",
    "trials = Trials()\n",
    "# run the hyperparameter optimization\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "# print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", best)\n",
    "# print the best score\n",
    "print(\"Best score:\", trials.best_trial['result']['loss'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas\n",
    "- check label encoding vs one hor encoding\n",
    "- hyperparamter tuning\n",
    "- use shape for neuronal network\n",
    "- feauture importance\n",
    "\n",
    "- Feauture engineering?\n",
    "- oversampling?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
